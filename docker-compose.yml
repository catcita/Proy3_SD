version: '3.8'

services:
  # --- Capa de Aplicación ---
  app2:
    build: ./app2
    deploy:
      replicas: 2
    env_file:
      - .env
    environment:
      - MIDDLEWARE_TYPE=tcp_server
      - APP2_DB_HOST=app2_proxysql
      - APP2_DB_PORT=6033
      - MIDDLEWARE_NOTIFY_HOST=172.18.0.1
    volumes:
      - ./app2:/usr/src/app
    depends_on:
      app2_proxysql:
        condition: service_healthy
    networks:
      - app2_network

  app2_nginx:
    image: nginx:1.25.4
    volumes:
      - ./app2_nginx_lb/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "${APP2_RUN_PORT}:80"
      - "${APP2_SOCKET_PORT:-6002}:6002"
    depends_on:
      - app2
    networks:
      - app2_network

  # --- Capa de Enrutamiento y HA ---
  app2_proxysql:
    image: proxysql/proxysql:2.5.4
    volumes:
      - ./app2_proxysql/conf/proxysql.cnf:/etc/proxysql.cnf
    ports:
      - "6032:6032"
      - "6033:6033"
    healthcheck:
      test: ["CMD", "mysql", "-h127.0.0.1", "-P6032", "-uadmin", "-padmin", "-e", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - app2_db1
      - app2_db2
      - app2_db3
    networks:
      - app2_network

  app2_orchestrator:
    build: ./app2_orchestrator
    ports:
      - "3000:3000"
    volumes:
      - ./app2_orchestrator/orchestrator.conf.json:/etc/orchestrator.conf.json
      - ./app2_orchestrator/scripts/:/scripts/
      - app2_orchestrator_data:/var/lib/orchestrator
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      - app2_db1
    networks:
      - app2_network
      
  app2_replication_setup:
    build: ./app2_orchestrator
    environment:
      - ORCHESTRATOR_API=http://app2_orchestrator:3000/api
    volumes:
      - ./app2_scripts/setup_replication.sh:/setup_replication.sh
    command: ["/bin/sh", "-c", "chmod +x /setup_replication.sh && /setup_replication.sh"]
    depends_on:
      app2_orchestrator:
        condition: service_healthy
      app2_db1:
        condition: service_healthy
      app2_db2:
        condition: service_healthy
      app2_db3:
        condition: service_healthy
    networks:
      - app2_network

  # --- Capa de Datos (Cluster Master-Slave) ---
  app2_db1:
    image: mariadb:10.5.25
    command: --server-id=1 --report-host=app2_db1
    environment:
      - MYSQL_ROOT_PASSWORD=${APP2_DB_ROOT_PASSWORD}
      - MYSQL_DATABASE=${APP2_DB_NAME}
      - MYSQL_USER=${APP2_DB_USER}
      - MYSQL_PASSWORD=${APP2_DB_PASSWORD}
    volumes:
      - ./app2_mariadb/conf/my.cnf:/etc/mysql/my.cnf
      - ./app2_mariadb/init_cluster.sql:/docker-entrypoint-initdb.d/init_cluster.sql
      - ./app2_mariadb/init.sql:/docker-entrypoint-initdb.d/init_app.sql
      - app2_db1_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u${APP2_DB_USER}", "-p${APP2_DB_PASSWORD}"]
      interval: 5s
      retries: 5
    networks:
      - app2_network

  app2_db2:
    image: mariadb:10.5.25
    command: --server-id=2 --report-host=app2_db2
    environment:
      - MYSQL_ROOT_PASSWORD=${APP2_DB_ROOT_PASSWORD}
      - MYSQL_DATABASE=${APP2_DB_NAME}
      - MYSQL_USER=${APP2_DB_USER}
      - MYSQL_PASSWORD=${APP2_DB_PASSWORD}
    volumes:
      - ./app2_mariadb/conf/my.cnf:/etc/mysql/my.cnf
      - ./app2_mariadb/init_cluster.sql:/docker-entrypoint-initdb.d/init_cluster.sql
      - ./app2_mariadb/init.sql:/docker-entrypoint-initdb.d/init_app.sql
      - app2_db2_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u${APP2_DB_USER}", "-p${APP2_DB_PASSWORD}"]
      interval: 5s
      retries: 5
    networks:
      - app2_network

  app2_db3:
    image: mariadb:10.5.25
    command: --server-id=3 --report-host=app2_db3
    environment:
      - MYSQL_ROOT_PASSWORD=${APP2_DB_ROOT_PASSWORD}
      - MYSQL_DATABASE=${APP2_DB_NAME}
      - MYSQL_USER=${APP2_DB_USER}
      - MYSQL_PASSWORD=${APP2_DB_PASSWORD}
    volumes:
      - ./app2_mariadb/conf/my.cnf:/etc/mysql/my.cnf
      - ./app2_mariadb/init_cluster.sql:/docker-entrypoint-initdb.d/init_cluster.sql
      - ./app2_mariadb/init.sql:/docker-entrypoint-initdb.d/init_app.sql
      - app2_db3_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u${APP2_DB_USER}", "-p${APP2_DB_PASSWORD}"]
      interval: 5s
      retries: 5
    networks:
      - app2_network

  # ========================================
  # INFRAESTRUCTURA COMÚN
  # ========================================
  etcd:
    image: quay.io/coreos/etcd:v3.5.0
    container_name: ticketflow_etcd
    environment:
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_ENABLE_V2=true
    ports:
      - "2379:2379"
    networks:
      - ticketflow_network

  rabbitmq:
    image: rabbitmq:3-management
    container_name: ticketflow_rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - ticketflow_network

  # ========================================
  # APP1 - GESTOR DE RESERVAS (Go + Patroni)
  # ========================================
  nginx:
    image: nginx:alpine
    container_name: app1_nginx_lb
    ports:
      - "8083:80"
    volumes:
      - ./app1/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app1_replica1
      - app1_replica2
    networks:
      - ticketflow_network

  app1_replica1:
    build:
      context: ./app1
      dockerfile: Dockerfile
    container_name: app1_replica1
    environment:
      - INSTANCE_NAME=app1_replica1
      - PORT=8080
      - DB_HOST=haproxy
      - DB_USER=postgres
      - DB_PASSWORD=postgres123
      - DB_NAME=ticketflow
    depends_on:
      - haproxy
    networks:
      - ticketflow_network
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 5s
      retries: 3

  app1_replica2:
    build:
      context: ./app1
      dockerfile: Dockerfile
    container_name: app1_replica2
    environment:
      - INSTANCE_NAME=app1_replica2
      - PORT=8080
      - DB_HOST=haproxy
      - DB_USER=postgres
      - DB_PASSWORD=postgres123
      - DB_NAME=ticketflow
    depends_on:
      - haproxy
    networks:
      - ticketflow_network
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 5s
      retries: 3

  # --- Patroni Cluster ---
  patroni_master:
    build: ./app1/db
    container_name: app1_patroni_master
    environment:
      - PATRONI_NAME=patroni_master
      - PATRONI_ETCD_HOST=etcd:2379
    volumes:
      - patroni_master_data:/var/lib/postgresql/data/patroni
    networks:
      - ticketflow_network
    depends_on:
      - etcd

  patroni_slave:
    build: ./app1/db
    container_name: app1_patroni_slave
    environment:
      - PATRONI_NAME=patroni_slave
      - PATRONI_ETCD_HOST=etcd:2379
    volumes:
      - patroni_slave_data:/var/lib/postgresql/data/patroni
    networks:
      - ticketflow_network
    depends_on:
      - etcd
      - patroni_master

  haproxy:
    image: haproxy:2.8-alpine
    container_name: app1_haproxy
    ports:
      - "5432:5432" # Postgres Master
      - "5433:5433" # Postgres Replica
      - "3307:3306" # MariaDB Proxy (Mapped to 3307 on host to avoid conflict if local mysql running)
      - "7001:7000" # Stats
    volumes:
      - ./app1/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - ticketflow_network
    depends_on:
      - patroni_master
      - patroni_slave
      # I removed the dependency on mariadb_master here because it will be managed by app2_proxysql

  # ========================================
  # APP3 & MIDDLEWARE
  # ========================================
  app3:
    build: ./app3
    container_name: ticketflow_app3
    ports:
      - "5003:5000"
    environment:
      - RABBITMQ_HOST=rabbitmq
    depends_on:
      - rabbitmq
    networks:
      - ticketflow_network

  middleware:
    build: ./middleware
    container_name: ticketflow_middleware
    ports:
      - "8000:8000"
    environment:
      - RABBITMQ_HOST=rabbitmq
    depends_on:
      - rabbitmq
    networks:
      - ticketflow_network

volumes:
  app2_db1_data:
  app2_db2_data:
  app2_db3_data:
  app2_orchestrator_data:
  patroni_master_data:
  patroni_slave_data:
  rabbitmq_data:

networks:
  app2_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.18.0.0/16
          gateway: 172.18.0.1
  ticketflow_network:
    driver: bridge